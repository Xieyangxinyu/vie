{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "from sklearn import linear_model\n",
    "import sys, os\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import time\n",
    "\n",
    "from utils_mondrian import sample_cut, errors_regression\n",
    "\n",
    "stable_sigmoid = lambda x: jnp.exp(jax.nn.log_sigmoid(x))\n",
    "\n",
    "def evaluate_all_lifetimes(X, y, X_test, y_test, M, lifetime_max, delta,\n",
    "                           validation=False, weights_from_lifetime=None):\n",
    "    \"\"\"\n",
    "    Sweeps through Mondrian kernels with all lifetime in [0, lifetime_max]. This can be used to (1) construct a Mondrian\n",
    "    feature map with lifetime lifetime_max, to (2) find a suitable lifetime (inverse kernel width), or to (3) compare\n",
    "    Mondrian kernel to Mondrian forest across lifetimes.\n",
    "    :param X:                       training inputs\n",
    "    :param y:                       training regression targets\n",
    "    :param X_test:                  test inputs\n",
    "    :param y_test:                  test regression targets\n",
    "    :param M:                       number of Mondrian trees\n",
    "    :param lifetime_max:            terminal lifetime\n",
    "    :param delta:                   ridge regression regularization hyperparameter\n",
    "    :param validation:              flag indicating whether a validation set should be created by halving the test set\n",
    "    :param weights_from_lifetime:   lifetime at which forest and kernel learned weights should be saved\n",
    "    :return: dictionary res containing all results\n",
    "    \"\"\"\n",
    "\n",
    "    N, D = np.shape(X)\n",
    "    N_test = np.shape(X_test)[0]\n",
    "    X_all = np.array(np.r_[X, X_test])\n",
    "    N_all = N + N_test\n",
    "    \n",
    "    y = np.squeeze(y)\n",
    "    y_test = np.squeeze(y_test)\n",
    "\n",
    "    # subtract target means\n",
    "    y_mean = np.mean(y)\n",
    "    y_train = y - y_mean\n",
    "\n",
    "    # initialize sparse feature matrix\n",
    "    indptr = range(0, M * N_all + 1, M)\n",
    "    indices = list(range(M)) * N_all\n",
    "    data = np.ones(N_all * M)\n",
    "    Z_all = scipy.sparse.csr_matrix((data, indices, indptr), shape=(N_all, M))\n",
    "    feature_from_repetition = list(range(M))\n",
    "    C = M\n",
    "\n",
    "    # bounding box for all datapoints used to sample first cut in each tree\n",
    "    feature_data = [np.array(range(N_all)) for _ in range(M)]\n",
    "    lX = np.min(X_all, 0)\n",
    "    uX = np.max(X_all, 0)\n",
    "\n",
    "    # event = tuple (time, tree, feature, dim, loc), where feature is the index of feature being split\n",
    "    events = []\n",
    "    active_features = []\n",
    "    cuts = []\n",
    "    feature_indices = []\n",
    "    active_features_in_tree = [[] for _ in range(M)]\n",
    "    for m in range(M):\n",
    "        cut_time, dim, loc = sample_cut(lX, uX, 0.0)\n",
    "        if cut_time < lifetime_max:\n",
    "            heapq.heappush(events, (cut_time, m, m, dim, loc))\n",
    "        active_features.append(m)\n",
    "        active_features_in_tree[m].append(m)\n",
    "\n",
    "    # iterate through birth times in increasing order\n",
    "    list_times = []\n",
    "    w_kernel = np.zeros(M)\n",
    "    list_kernel_error_train = []\n",
    "    if validation:\n",
    "        list_kernel_error_validation = []\n",
    "    list_kernel_error_test = []\n",
    "\n",
    "    parent = {}\n",
    "\n",
    "    while len(events) > 0:\n",
    "        (birth_time, m, c, dim, loc) = heapq.heappop(events)\n",
    "        list_times.append(birth_time)\n",
    "        \n",
    "        # construct new feature\n",
    "        Xd = X_all[feature_data[c], dim]\n",
    "        feature_l = (feature_data[c])[Xd <= loc]\n",
    "        feature_r = (feature_data[c])[Xd  > loc]\n",
    "        feature_data.append(feature_l)\n",
    "        feature_data.append(feature_r)\n",
    "\n",
    "        active_features.remove(c)\n",
    "        active_features_in_tree[m].remove(c)\n",
    "        active_features.append(C + 0)\n",
    "        active_features.append(C + 1)\n",
    "        active_features_in_tree[m].append(C + 0)\n",
    "        active_features_in_tree[m].append(C + 1)\n",
    "        feature_indices.append(dim)\n",
    "        #feature_indices.append(dim)\n",
    "        cuts.append([loc, 0])\n",
    "        parent[C + 0] = c\n",
    "        parent[C + 1] = c\n",
    "        #cuts.append([loc, 1])\n",
    "\n",
    "        # move datapoints from split feature to child features\n",
    "        Z_all.indices[feature_l * M + m] = C + 0\n",
    "        Z_all.indices[feature_r * M + m] = C + 1\n",
    "        Z_all = scipy.sparse.csr_matrix((Z_all.data, Z_all.indices, Z_all.indptr), shape=(N_all, C + 2), copy=False)\n",
    "\n",
    "        # sample the cut for each child\n",
    "        lX_l = np.min(X_all[feature_l, :], axis=0)\n",
    "        uX_l = np.max(X_all[feature_l, :], axis=0)\n",
    "        cut_time_l, dim_l, loc_l = sample_cut(lX_l, uX_l, birth_time)\n",
    "        lX_r = np.min(X_all[feature_r, :], axis=0)\n",
    "        uX_r = np.max(X_all[feature_r, :], axis=0)\n",
    "        cut_time_r, dim_r, loc_r = sample_cut(lX_r, uX_r, birth_time)\n",
    "\n",
    "        # add new cuts to heap\n",
    "        if cut_time_l < lifetime_max:\n",
    "            heapq.heappush(events, (cut_time_l, m, C + 0, dim_l, loc_l))\n",
    "        if cut_time_r < lifetime_max:\n",
    "            heapq.heappush(events, (cut_time_r, m, C + 1, dim_r, loc_r))\n",
    "\n",
    "        feature_from_repetition.append(m)\n",
    "        feature_from_repetition.append(m)\n",
    "        C += 2\n",
    "\n",
    "        # update Mondrian kernel predictions\n",
    "        w_kernel = np.append(w_kernel, [w_kernel[c], w_kernel[c]])\n",
    "        w_kernel[c] = 0\n",
    "        Z_train = Z_all[:N]\n",
    "        Z_test = Z_all[N:]\n",
    "\n",
    "        clf = linear_model.SGDRegressor(alpha=delta, fit_intercept=False)\n",
    "        clf.fit(Z_train, y_train, coef_init=w_kernel)\n",
    "        w_kernel = clf.coef_\n",
    "\n",
    "        if weights_from_lifetime is not None and birth_time <= weights_from_lifetime:\n",
    "            w_kernel_save = np.array(w_kernel[active_features])\n",
    "        y_hat_train = y_mean + Z_train.dot(w_kernel)\n",
    "        y_hat_test = y_mean + Z_test.dot(w_kernel)\n",
    "        if validation:\n",
    "            error_train, error_validation =\\\n",
    "                errors_regression(y, y_test[:(N_test/2)], y_hat_train, y_hat_test[:(N_test/2)])\n",
    "            error_train, error_test =\\\n",
    "                errors_regression(y, y_test[(N_test/2):], y_hat_train, y_hat_test[(N_test/2):])\n",
    "            list_kernel_error_validation.append(error_validation)\n",
    "        else:\n",
    "            error_train, error_test = errors_regression(y, y_test, y_hat_train, y_hat_test)\n",
    "        list_kernel_error_train.append(error_train)\n",
    "        list_kernel_error_test.append(error_test)\n",
    "\n",
    "\n",
    "        sys.stdout.write(\"\\rTime: %.2E / %.2E (C = %d, test error = %.3f)\" % (birth_time, lifetime_max, C, error_test/100))\n",
    "        sys.stdout.flush()\n",
    "    \n",
    "    sys.stdout.write(\"\\n\")\n",
    "\n",
    "    cuts = np.array(cuts)\n",
    "    \n",
    "    path_map = np.zeros(shape=(C, M))\n",
    "    for i in range(M, C):\n",
    "        v = np.copy(path_map[:, parent[i]])\n",
    "        v[i] = 1\n",
    "        v = v[...,None]\n",
    "        path_map = np.hstack((path_map, v))\n",
    "    path_map = path_map[M:, M:]\n",
    "\n",
    "    c = 1\n",
    "    threshold = cuts[:, 0]\n",
    "\n",
    "    def predict(x, Z_all, feature_indices, threshold, beta):\n",
    "        hidden_features = x[:, feature_indices]\n",
    "        right_indicator = stable_sigmoid(c * (hidden_features - threshold))\n",
    "        left_indicator = 1 - right_indicator\n",
    "        soft_indicator = np.ravel([left_indicator,right_indicator],order=\"F\").reshape(len(X_all),len(feature_indices)*2)\n",
    "        F_leaf = jnp.multiply(soft_indicator, Z_all)\n",
    "        F_leaf = jnp.matmul(F_leaf, path_map)\n",
    "        return jnp.dot(F_leaf, beta)\n",
    "    \n",
    "    y_hat = predict(X_all, Z_all[:, M:].todense(), feature_indices, threshold, w_kernel[M:])\n",
    "    f = jax.jit(jax.vmap(jax.vmap(predict, in_axes=(None, 0, 0, 0, 0), out_axes=0),\n",
    "                         in_axes=(0, None, None, None, None), out_axes=0))\n",
    "\n",
    "    grad_fs = jax.jit(jax.vmap(jax.vmap(jax.vmap(jax.grad(predict, argnums=0),\n",
    "                                                 in_axes=(None, 0, 0, 0, 0), out_axes=0),\n",
    "                                        in_axes=(0, None, None, None, None), out_axes=0),\n",
    "                               in_axes=(None, None, None, None, 1), out_axes=0))\n",
    "    \n",
    "    #print(np.array(grad_fs(X_all, Z_all, feature_indices, threshold, w_kernel[M:])))\n",
    "\n",
    "    y_hat_train = y_hat[:N]\n",
    "    y_hat_test = y_hat[N:]\n",
    "    error_train, error_test = errors_regression(y, y_test, y_hat_train, y_hat_test)\n",
    "    print(error_test/100)\n",
    "\n",
    "    # this function returns a dictionary with all values of interest stored in it\n",
    "    results = {'times': list_times, 'Z': Z_all, 'feature_from_repetition': np.array(feature_from_repetition)}\n",
    "    results['kernel_train'] = list_kernel_error_train\n",
    "    results['kernel_test'] = list_kernel_error_test\n",
    "    if validation:\n",
    "        results['kernel_validation'] = list_kernel_error_validation\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 8.28E-03 / 1.00E-02 (C = 26, test error = 1.045)\n",
      "1.0109957432928223\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "\n",
    "def prepare_training_data(data, n_obs):\n",
    "    df_train = data.head(n_obs)\n",
    "    df_test = data.tail(40)\n",
    "\n",
    "    x_train, y_train, f_train = df_train, df_train.pop(\"y\"), df_train.pop(\"f\")\n",
    "    x_test, y_test, f_test = df_test, df_test.pop(\"y\"), df_test.pop(\"f\")\n",
    "\n",
    "\n",
    "    x_train = x_train.to_numpy()\n",
    "    #x_train = x_train[:,:10]\n",
    "    x_test = x_test.to_numpy()\n",
    "    #z_test = np.copy(x_test)\n",
    "    #z_test[:, 1] = z_test[:, 1] + 1\n",
    "    #x_test = np.vstack([x_test, z_test])\n",
    "    \n",
    "    #x_test = x_test[:,:10]\n",
    "\n",
    "    y_train = y_train.to_numpy().reshape(-1, 1).ravel()\n",
    "    y_test = y_test.to_numpy().reshape(-1, 1).ravel()\n",
    "    #y_test = np.hstack([y_test, y_test])\n",
    "\n",
    "    scaler = preprocessing.StandardScaler().fit(x_train)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_path = os.path.join(\"./datasets/\")\n",
    "    \n",
    "    dataset_name = 'cont' # @param ['cat', 'cont', 'adult', 'heart', 'mi'] \n",
    "    outcome_type = 'rbf' # @param ['linear', 'rbf', 'matern32', 'complex']\n",
    "    n_obs = 200 # @param [100, 200, 500, 1000]\n",
    "    dim_in = 25 # @param [25, 50, 100, 200]\n",
    "    rep = 1 # @param \n",
    "\n",
    "    data_file = f\"{outcome_type}_n{n_obs}_d{dim_in}_i{rep}.csv\"\n",
    "    data_file_path = os.path.join(data_path, dataset_name, data_file)\n",
    "    print(f\"Data '{data_file}'\", end='\\t', flush=True)\n",
    "\n",
    "    data = pd.read_csv(data_file_path, index_col=0)\n",
    "    x_train, y_train, x_test, y_test = prepare_training_data(data, n_obs)\n",
    "\n",
    "\n",
    "    M = 10                      # number of Mondrian trees to use\n",
    "    lifetime_max = 0.01          # terminal lifetime\n",
    "    weights_lifetime = 2*1e-6   # lifetime for which weights should be plotted\n",
    "    delta = 0.01              # ridge regression delta\n",
    "    evaluate_all_lifetimes(x_train, y_train, x_test, y_test, M, lifetime_max, delta,\n",
    "                                weights_from_lifetime=weights_lifetime)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d59b506a0e703b59755888ed29552127d9b4544333d10d3ff144388b8386bf11"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
